{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19604435",
   "metadata": {},
   "source": [
    "# A. Course Outline\n",
    "    \n",
    "    1. Diagnose dirty data\n",
    "    2. Side effects of dirty data\n",
    "    3. Clean data\n",
    "    \n",
    "Chapter 1 - Common Data Problems\n",
    "\n",
    "    Data Type Constraints\n",
    "        Strings\n",
    "        Numeric data\n",
    "        \n",
    "    Data Range Constraints\n",
    "        Out of range data\n",
    "        Out of range dates\n",
    "        \n",
    "    Uniqueness \n",
    "        Finding duplicates\n",
    "        Treating duplicates\n",
    "        \n",
    "Chapter 2 - Text and categorical data problems    \n",
    "\n",
    "    Membership constraints\n",
    "        Finding inconsistent categories\n",
    "        Treating them with joins\n",
    "        \n",
    "    Categorical values\n",
    "        Finding inconsistent categories\n",
    "        Collapsing them into less\n",
    "    \n",
    "    Cleaning Text\n",
    "        Unifying formats\n",
    "        Finding lengths\n",
    "\n",
    "Chapter 3 -Advanced ata problems\n",
    "\n",
    "    Uniformity\n",
    "        Unifying currency formats\n",
    "        Unifying date formats\n",
    "    \n",
    "    Cross field validation\n",
    "        Summing across rows\n",
    "        Building assert functions\n",
    "        \n",
    "    Completeness\n",
    "        Finding missing data\n",
    "        Treating them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb896d5d",
   "metadata": {},
   "source": [
    "# 1. Common Data Problems \n",
    "\n",
    "Data Science workflow\n",
    "\n",
    "    Access data -> Explore and Process Data -> Extract Insights -> Report Insights \n",
    "    \n",
    "Why do we need to clean data?\n",
    "\n",
    "    Dirty data can appear because of duplicate values, mis-spellings, data type parsing errors and legacy systems. \n",
    "    \n",
    "    Without making sure that data is properly cleaned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated.\n",
    "    \n",
    "Data Type Constraints \n",
    "\n",
    "    We could be working with text data, integers, decimals, dates, zip codes, and others. Luckily, Python has specific data type objects for various data types that you're probably familiar with by now. This makes it much easier to manipulate these various data types in Python. As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our analysis.\n",
    "    \n",
    "        Text: str\n",
    "        Integers: int\n",
    "        Decimals: float\n",
    "        Binary: bool \n",
    "        Dates: datetime\n",
    "        Categories: category \n",
    "    \n",
    "Strings to integers\n",
    "\n",
    "    sales.dtypes #Get data types of columns \n",
    "    sales.info() #Get dataframe information \n",
    "    sales[\"Revenue\"].sum() - it is a string so it will just append. \n",
    "    \n",
    "    Remove $ from revenue column\n",
    "        sales[\"Revenue\"].sales[\"Revenue\"].str.strip(\"$\")\n",
    "        sales[\"Revenue\"].sales[\"Revenue\"].astype(\"int\")\n",
    "        \n",
    "The assert statement\n",
    "\n",
    "    verify that revenue is now an integer\n",
    "    \n",
    "        assert sales[\"Revenue\"].dtype == 'int\"\n",
    "        \n",
    "        Ex. assert 1 + 1 = 2 #true\n",
    "\n",
    "Numeric or categorical?\n",
    "\n",
    "#Convert to categorical data when needed\n",
    "\n",
    "df[\"marriage_status\"] = df[\"marriage_status\"].astype(\"category\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b4990",
   "metadata": {},
   "source": [
    "### 1.1 Common Data Types\n",
    "\n",
    "When working with new data, you should always check the data types of your columns using the .dtypes attribute or the .info() method which you'll see in the next exercise. Often times, you'll run into columns that should be converted to different data types before starting any analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba0b6a",
   "metadata": {},
   "source": [
    "#### 1.1.1 Numeric data or? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Print the information of ride_sharing\\nprint(ride_sharing.info())\\n\\n# Print summary statistics of user_type column\\nprint(ride_sharing[\\'user_type\\'].describe())\\n\\n# Convert user_type from integer to category\\nride_sharing[\\'user_type_cat\\'] = ride_sharing[\\'user_type\\'].astype(\"category\")\\n\\n# Write an assert statement confirming the change\\nassert ride_sharing[\\'user_type_cat\\'].dtype == \"category\"\\n\\n# Print new summary statistics \\nprint(ride_sharing[\\'user_type_cat\\'].describe())\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype(\"category\")\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == \"category\"\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c265a",
   "metadata": {},
   "source": [
    "#### 1.1.2 Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it in order to extract relevant statistical summaries that shed light on the distribution of user_type.\n",
    "\n",
    "Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(\"int\")\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing[\"duration_time\"].mean())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dae085",
   "metadata": {},
   "source": [
    "## 1.2  Data range contraints\n",
    "\n",
    "Motivation 1\n",
    "\n",
    " Imagine we have a dataset of movies with their respective average rating from a streaming service. The rating can be any integer between 1 an 5.\n",
    " \n",
    " After creating a histogram with maptlotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range. This is most likely an error in data collection or parsing, where a variable is well beyond its range and treating it is essential to have accurate analysis.\n",
    " \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(movies['avg_rating\"])\n",
    "    \n",
    "Motivation 2: Can future sign ups exist? \n",
    "\n",
    "    import datetime as dt\n",
    "    today_date = dt.date.today()\n",
    "    user_signups[user_signups[\"subscription_date\"] > today_date]\n",
    "    \n",
    "    \n",
    "How to deal with out of range data?\n",
    "\n",
    "    1. DROP DATA - The simplest option is to drop the data. However, depending on the size of your out of range data, you could be losing out on essential information. As a rule of thumb, only DROP DATA when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values.\n",
    "    \n",
    "        import pandas as pd\n",
    "    \n",
    "        #Output movies with rating > 5 \n",
    "            movies[movies[\"avg_rating\"] > 5]\n",
    "        \n",
    "        #Drop values using filtering\n",
    "            movies = movies[movies[\"avg_rating\"] <=5]\n",
    "    \n",
    "        #Drop values using drop\n",
    "    \n",
    "            movies.drop(movies[movies[\"avg_rating\"] > 5].index, inplace = True)\n",
    "         \n",
    "        #Assert results\n",
    "    \n",
    "            assert movies[\"avg_ratinng\"].max() <=5\n",
    "    \n",
    "    2. Set custom minimums or maximums to your columns\n",
    "    \n",
    "        Convert avg_ratying > 5 to 5 \n",
    "        \n",
    "            movies.loc[movies[\"avg_rating\"] > 5, \"avg_rating\"] = 5\n",
    "            assert movies[\"avg_ratinng\"].max() <=5\n",
    "            \n",
    "    3. Set the data to missing, and impute it\n",
    "    4. We could also, dependent on the business assumptions behind our data, assign a custom value for any values of our data that go beyond a certain range.\n",
    "    \n",
    "    \n",
    "Date range example\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "#Output data types\n",
    "\n",
    "    user_signups.dtypes\n",
    "\n",
    "#Convert to date\n",
    "    user_signups[\"subscription_date\"] = pd.to_datetime(user_signups[\"subscription_date\"]).dt.date\n",
    "    \n",
    "#today date\n",
    "\n",
    "    today_date = dt.date.today()\n",
    "    \n",
    "#Drop using filtering\n",
    "    \n",
    "    user_signups =  user_signups[user_signups[\"subscription_date\"] < today_date]\n",
    "    \n",
    "#Drop using .drop\n",
    "\n",
    "    user_signups.drop(user_signups[user_signups[\"subscription_date\"] > today_date].index, inplace = True)\n",
    "        \n",
    "#Hardcode dates with upper limit\n",
    "    \n",
    "    user_signups.loc(user_signups[user_signups[\"subscription_date\"] > today_date, \"subscription_date\"] = today_date\n",
    "\n",
    "#Assert \n",
    "\n",
    "    assert user_signups.subscription_date.max().date() <= today_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ce010",
   "metadata": {},
   "source": [
    "#### 1.2.1 Tire size constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fbecf",
   "metadata": {},
   "source": [
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11a9459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>duration</th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_A_name</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>station_B_name</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_birth_year</th>\n",
       "      <th>user_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12 minutes</td>\n",
       "      <td>81</td>\n",
       "      <td>Berry St at 4th St</td>\n",
       "      <td>323</td>\n",
       "      <td>Broadway at Kearny</td>\n",
       "      <td>5480</td>\n",
       "      <td>2</td>\n",
       "      <td>1959</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24 minutes</td>\n",
       "      <td>3</td>\n",
       "      <td>Powell St BART Station (Market St at 4th St)</td>\n",
       "      <td>118</td>\n",
       "      <td>Eureka Valley Recreation Center</td>\n",
       "      <td>5193</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8 minutes</td>\n",
       "      <td>67</td>\n",
       "      <td>San Francisco Caltrain Station 2  (Townsend St...</td>\n",
       "      <td>23</td>\n",
       "      <td>The Embarcadero at Steuart St</td>\n",
       "      <td>3652</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>16</td>\n",
       "      <td>Steuart St at Market St</td>\n",
       "      <td>28</td>\n",
       "      <td>The Embarcadero at Bryant St</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11 minutes</td>\n",
       "      <td>22</td>\n",
       "      <td>Howard St at Beale St</td>\n",
       "      <td>350</td>\n",
       "      <td>8th St at Brannan St</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    duration  station_A_id  \\\n",
       "0           0  12 minutes            81   \n",
       "1           1  24 minutes             3   \n",
       "2           2   8 minutes            67   \n",
       "3           3   4 minutes            16   \n",
       "4           4  11 minutes            22   \n",
       "\n",
       "                                      station_A_name  station_B_id  \\\n",
       "0                                 Berry St at 4th St           323   \n",
       "1       Powell St BART Station (Market St at 4th St)           118   \n",
       "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
       "3                            Steuart St at Market St            28   \n",
       "4                              Howard St at Beale St           350   \n",
       "\n",
       "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
       "0               Broadway at Kearny     5480          2             1959   \n",
       "1  Eureka Valley Recreation Center     5193          2             1965   \n",
       "2    The Embarcadero at Steuart St     3652          3             1993   \n",
       "3     The Embarcadero at Bryant St     1883          1             1979   \n",
       "4             8th St at Brannan St     4626          2             1994   \n",
       "\n",
       "  user_gender  \n",
       "0        Male  \n",
       "1        Male  \n",
       "2        Male  \n",
       "3        Male  \n",
       "4        Male  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import needed datasets\n",
    "\n",
    "import pandas as pd\n",
    "ride_sharing = pd.read_csv(\"E:/Upskilling/DataCamp/Data Scientist (Python)/20_Cleaning Data in Python/Datasets/ride_sharing_new.csv\")\n",
    "\n",
    "ride_sharing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c21a793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Convert tire_sizes to integer\\nride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\\n\\n# Set all values above 27 to 27\\nride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\\n\\n# Reconvert tire_sizes back to categorical\\nride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\\n\\n# Print tire size description\\nprint(ride_sharing['tire_sizes'].describe())\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e49f9",
   "metadata": {},
   "source": [
    "#### 1.2.2 Back to the future\n",
    "\n",
    "DataFrame has been updated to register each ride's date. This information is stored in the ride_date column of the type object, which represents strings in pandas.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the ride_date column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert ride_date to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e214b0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Convert ride_date to date\\nride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\\n\\n# Save today's date\\ntoday = dt.date.today()\\n\\n# Set all in the future to today's date\\nride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\\n\\n# Print maximum of ride_dt column\\nprint(ride_sharing['ride_dt'].max())\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41325f",
   "metadata": {},
   "source": [
    "## 1.3 UNIQUENESS CONSTRAINTS \n",
    "\n",
    "What are duplicate values?\n",
    "\n",
    "Duplicate values can be diagnosed when we have the same exact information repeated across multiple rows, for a some or all columns in our DataFrame. In this example DataFrame containing the names, address, height, and weight of individuals, the rows presented have identical values across all columns.\n",
    "\n",
    "Why do they happen?\n",
    "\n",
    "    1. data entry and human errors\n",
    "    2. bugs and design errors\n",
    "    3. Join or merge data from various resources, which could retain duplicate values.\n",
    "\n",
    "How to find duplicate values?\n",
    "\n",
    "    #Get duplicates across all columns \n",
    "    \n",
    "        duplicates = height_weight.duplicated()\n",
    "        print(duplicates) #False True \n",
    "    \n",
    "    using the default .duplicated() without tweaking some of the arguments returns only the non-first complete duplicates across all columns.\n",
    "    \n",
    "     #Get duplicates rows\n",
    "     \n",
    "         duplicates = height_weight.duplicated()\n",
    "         height_weight[duplicates]\n",
    "     \n",
    "1. .duplicated() method\n",
    "\n",
    "    subset: list of column names to check for duplication \n",
    "    keep: whether to keep first (\"first\"), last (\"last\"), or all (False) duplicate values \n",
    "    \n",
    "Example: \n",
    "\n",
    "   #create a list of column names to check for duplication\n",
    "   \n",
    "       column_names = [\"first-name\", \"last_name\", \"address\"]\n",
    "       \n",
    "       duplicates = height_weight.duplicated(subset = column_names, keep = False) \n",
    "   \n",
    "       height_weight[duplicates[.sort_values(by = \"first_name\")\n",
    "   \n",
    "   \n",
    "2. .drop_duplicates() method \n",
    "\n",
    "    subset: list of column names to check for duplication \n",
    "    keep: whether to keep first (\"first\"), last (\"last\"), or all (False) duplicate values \n",
    "    inplace: Drop duplicated rows directly inside DataFrame without creating new object (True)\n",
    "    \n",
    "    \n",
    "    inplace argument which drops the duplicated values directly inside the height_weight DataFrame. Here we are dropping complete duplicates only, so it's not necessary nor advisable to set a subset, and since the keep argument takes in first as default, we can keep it as such. Note that we can also set it as last, but not as False as it would keep all duplicates.\n",
    "    \n",
    "    \n",
    "3. The .groupby() and .agg() methods\n",
    "\n",
    "    #Group by column names and produce statistical summaries \n",
    "\n",
    "       column_names = [\"first-name\", \"last_name\", \"address\"]\n",
    "   \n",
    "       summaries = {'height': 'max', 'weight': 'mean'}\n",
    "       \n",
    "       height_weight =  height_weight.groupby(by = column_names).agg(summaries).reset-index()\n",
    "\n",
    "    #Make sure aggregation is done\n",
    "\n",
    "        duplicates = height_weight.duplicated(subset = column_names, keep = False) \n",
    "        \n",
    "        height_weight[duplicates].sort_values(by = \"first_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d042f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Find duplicates\\nduplicates = ride_sharing.duplicated(\"ride_id\", keep = False)\\n\\n# Sort your duplicated rides\\nduplicated_rides = ride_sharing[duplicates].sort_values(\\'ride_id\\')\\n\\n# Print relevant columns of duplicated_rides\\nprint(duplicated_rides[[\\'ride_id\\',\\'duration\\',\\'user_birth_year\\']])\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(\"ride_id\", keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135a7d9",
   "metadata": {},
   "source": [
    "Notice that rides 33 and 89 are incomplete duplicates, whereas the remaining are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13993f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Drop complete duplicates from ride_sharing\\nride_dup = ride_sharing.drop_duplicates()\\n\\n# Create statistics dictionary for aggregation function\\nstatistics = {'user_birth_year': 'min', 'duration': 'mean'}\\n\\n# Group by ride_id and compute new statistics\\nride_unique = ride_dup.groupby(by = 'ride_id').agg(statistics).reset_index()\\n\\n# Find duplicated values again\\nduplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\\nduplicated_rides = ride_unique[duplicates == True]\\n\\n# Assert duplicates are processed\\nassert duplicated_rides.shape[0] == 0\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby(by = 'ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0ad7f",
   "metadata": {},
   "source": [
    "# 2. Text and categorical data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942fd611",
   "metadata": {},
   "source": [
    "### 2.1 Categories and membership constraints\n",
    "\n",
    "Categories and membership constraints\n",
    " \n",
    "    Categorical data represent variables that represent predefined finite set of categories. Examples of this range from marriage status, household income categories, loan status and others. To run machine learning models on categorical data, they are often coded as numbers. Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.\n",
    "\n",
    "Why could we have these problems?\n",
    "    \n",
    "    inconsistencies in our categorical data for a variety of reasons. This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors, \n",
    "    \n",
    "How do we treat these problems?\n",
    "\n",
    "    1. Drop the rows with incorrect categories. \n",
    "    2. Remapping incorrect categories to correct ones\n",
    "    3, Inferring categories \n",
    "\n",
    "An example\n",
    "\n",
    "    #Read study data and print it\n",
    "    \n",
    "        study_data = pd.read_csv(\"study.csv\")\n",
    "        study_data \n",
    "        \n",
    "    #Correct possible error\n",
    "        categories - list of blood data type categories \n",
    "\n",
    "A note on joins\n",
    "\n",
    "    1. Anti join - What is in A and not in B\n",
    "    2. Inner join - Both in A and B \n",
    "\n",
    "A left anti join on blood types - return only rows containing Z+ \n",
    "\n",
    "    left table - study data only (Z+)\n",
    "    right table - categories (all correct blood type) \n",
    "\n",
    "A inner join on blood types - Both present in study and category \n",
    "\n",
    "Example\n",
    "\n",
    "    Finding categories \n",
    "    \n",
    "        inconsistent_categories = set(study_data[\"blood_type\"]).difference(categories[\"blood_type\"])\n",
    "        \n",
    "        We first get all inconsistent categories in the blood_type column of the study_data DataFrame. We do that by creating a set out of the blood_type column which stores its unique values, and use the difference method which takes in as argument the blood_type column from the categories DataFrame. This returns all the categories in blood_type that are not in categories. \n",
    "        \n",
    "        We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the isin method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones. \n",
    "        \n",
    "        #Get and print rows with inconsistent categories \n",
    "        \n",
    "        inconsistent_rows = study_data[\"blood_type\"].isin(inconsistent_categories)\n",
    "        study_data[inconsistent_rows] \n",
    "        \n",
    "        \n",
    "        We then subset the study_data DataFrame based on these boolean values, and voila we have our inconsistent data.\n",
    "        \n",
    "        consistent_data = study_data[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd3cf4",
   "metadata": {},
   "source": [
    "### 2.1.1 Finding consistency\n",
    "\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2156740",
   "metadata": {},
   "source": [
    "#### Print Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b1ea13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Print categories DataFrame\\nprint(categories)\\n\\n# Print unique values of survey columns in airlines\\nprint(\\'Cleanliness: \\', airlines[\\'cleanliness\\'].unique(), \"\\n\")\\nprint(\\'Safety: \\', airlines[\\'safety\\'].unique(), \"\\n\")\\nprint(\\'Satisfaction: \\', airlines[\\'satisfaction\\'].unique(), \"\\n\")\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414a604",
   "metadata": {},
   "source": [
    "#### Find inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4eff8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Find the cleanliness category in airlines not in categories\\ncat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\\n\\n# Find rows with that category\\ncat_clean_rows = airlines['cleanliness'].isin(cat_clean)\\n\\n# Print rows with inconsistent category\\nprint(airlines[cat_clean_rows])\\n\\n# Print rows with consistent categories only\\nprint(airlines[~cat_clean_rows])\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41c779",
   "metadata": {},
   "source": [
    "### 2.2 Categorial Variables\n",
    "\n",
    "What type of errors could we have?\n",
    "\n",
    "    When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.\n",
    "    \n",
    "    1. Value consistency\n",
    "        \n",
    "        Inconsistent fields: married, Maried, UNMARRIED, not married\n",
    "        Trailing white space: _married, ' married'\n",
    "        \n",
    "    2. Collapsing too many categories to few\n",
    "    \n",
    "        Creating new groups: 0-20k, 20-40k categories\n",
    "        Mapping groups to new ones: Mapping household income categories to 2, rich or poor\n",
    "        \n",
    "    \n",
    "Capitalization: married, Maried, UNMARRIED, not married\n",
    "\n",
    "    A common categorical data problem is having values that slightly differ because of capitalization. Not treating this could lead to misleading results when we decide to analyze our data, for example, let's assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization.\n",
    "    \n",
    "    marriage_status = demographivs[\"marital_status\"]\n",
    "    marriage_status.value_counts() #series only\n",
    "    \n",
    "    #Get value counts on dataframe\n",
    "    \n",
    "        marriage_status.groupby(\"marriage_status\"].count()   \n",
    "    \n",
    "    To deal with this, we can either capitalize or lowercase the marriage_status column. This can be done with the str-dot-upper() or dot-lower() functions respectively.\n",
    "    \n",
    "    \n",
    "    marriage_status[\"marriage_status\"] =  marriage_status[\"marriage_status\"].str.upper()\n",
    "    marriage_status[\"marriage_status\"].value_counts()\n",
    "    \n",
    "Trailing Spaces \n",
    "    \n",
    "    \n",
    "    Another common problem with categorical values are leading or trailing spaces. For example, imagine the same demographics DataFrame containing values with leading spaces. Here's what the counts of married vs unmarried people would look like. Note that there is a married category with a trailing space on the right, which makes it hard to spot on the output, as opposed to unmarried.\n",
    "    \n",
    "    \n",
    "    #Strip all spaces \n",
    "    \n",
    "          marriage_status[\"marriage_status\"] =  marriage_status[\"marriage_status\"].str.strip()\n",
    "          marriage_status[\"marriage_status\"].value_counts()\n",
    "    \n",
    "\n",
    "Collapsing data into categories\n",
    "\n",
    "    1. Create categories of data: income group column from income column\n",
    "    \n",
    "        # Using qcut():\n",
    "\n",
    "            import pandas as pd\n",
    "            group_names = [\"0-200k\", '200k-500k', '500k]\n",
    "            demographics['income_group] = pd.qcut(demographics['household_income'], q = 3, labels =   group_names]\n",
    "\n",
    "        # print income_group column \n",
    "\n",
    "            demographics[[\"inciome_group\", \"household_income\"]]\n",
    "\n",
    "\n",
    "        #using cut - create category ranges and names \n",
    "\n",
    "            ranges = [0,200000, 500000, np.inf] \n",
    "            group_names = [\"0-200k\", '200k-500k', '500k]\n",
    "\n",
    "            demographics['income_group] = pd.cut(demographics['household_income'], bins = ranges, labels =   group_names]\n",
    "\n",
    "            demographics[[\"inciome_group\", \"household_income\"]]        \n",
    "\n",
    "\n",
    "\n",
    "    2. Map categories to fewer ones: \n",
    "        \n",
    "        operating system column is: Microsoft, MacOS, IOS, Android, Linux\n",
    "        Convert to: DesktopOS, MobileOS\n",
    "        \n",
    "        mapping = {'Microsoft': 'DesktopOS', 'MacOS': 'DesktopOS', 'Linux': 'DesktopOS', \n",
    "                    'IOS': 'MobileOS', 'Android': 'MobileOS'}\n",
    "                    \n",
    "        devices['operating_system'] = devices[\"operating_system\"].replace(mapping)\n",
    "        devices['operating_system'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8af6c",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inconsistent categories\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "026041ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Print unique values of both columns\\nprint(airlines[\\'dest_region\\'].unique())\\nprint(airlines[\\'dest_size\\'].unique())\\n\\n# Lower dest_region column and then replace \"eur\" with \"europe\"\\nairlines[\\'dest_region\\'] = airlines[\\'dest_region\\'].str.lower() \\nairlines[\\'dest_region\\'] = airlines[\\'dest_region\\'].replace({\\'eur\\':\\'europe\\'})\\n\\n# Remove white spaces from `dest_size`\\nairlines[\\'dest_size\\'] = airlines[\\'dest_size\\'].str.strip()\\n\\n# Verify changes have been effected\\nprint(airlines[\"dest_region\"].unique())\\nprint(airlines[\"dest_region\"].unique())\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines[\"dest_region\"].unique())\n",
    "print(airlines[\"dest_region\"].unique())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551b857",
   "metadata": {},
   "source": [
    "#### 2.2.2 Remapping categories\n",
    "\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The airlines DataFrame contains the day and wait_min columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ea3785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Create ranges for categories\\nlabel_ranges = [0, 60, 180, np.inf]\\nlabel_names = [\\'short\\', \\'medium\\', \\'long\\']\\n\\n# Create wait_type column\\nairlines[\\'wait_type\\'] = pd.cut(airlines[\"wait_min\"], bins = label_ranges, \\n                                labels = label_names)\\n\\n# Create mappings and replace\\nmappings = {\\'Monday\\':\\'weekday\\', \\'Tuesday\\':\\'weekday\\', \\'Wednesday\\': \\'weekday\\', \\n            \\'Thursday\\': \\'weekday\\', \\'Friday\\': \\'weekday\\', \\n            \\'Saturday\\': \\'weekend\\', \\'Sunday\\': \\'weekend\\'}\\n\\nairlines[\\'day_week\\'] = airlines[\\'day\\'].replace(mappings)\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines[\"wait_min\"], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b0190",
   "metadata": {},
   "source": [
    "#### 2.3 Cleaning Text Data\n",
    "\n",
    "What is text data? \n",
    "\n",
    "    Text data is one of the most common types of data types. Examples of it range from names, phone numbers, addresses, emails and more.\n",
    "\n",
    "    Common text data problems\n",
    "\n",
    "        1. Data Inconsistency: +961717 or 009617\n",
    "        2. Fixed length violations: ex. passwords atleast 8 characters \n",
    "        3. Typos \n",
    "       \n",
    "#Replace \"+\" with \"00\"\n",
    "\n",
    "    phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"+\", \"00\") \n",
    "    phones \n",
    "\n",
    "#Replace phone numbers with lower than 10 digits to NaN\n",
    "\n",
    "    digits = phones[\"phone number\"].str.len()\n",
    "    phones.loc[digit < 10, \"phone number\"] = np.nan \n",
    "    phones \n",
    "\n",
    "Fixing the phone number column \n",
    "\n",
    "    #Find length of each row in phone number column \n",
    "    \n",
    "        sanity_check = phone['Phone_number'].str.len()\n",
    "   \n",
    "    #Assert minmum phone number length is 10 \n",
    "        assert sanity_check.min() >= 10\n",
    "    \n",
    "    #Assert all numbers do not have \"+\" or \"-\"\n",
    "        assert phone[\"Phone_number\"].str.contains(\"+|-\").any() == False \n",
    "        \n",
    "        any method - returns True if any element in the output of our dot-str-contains is True, and test whether the it returns False.\n",
    "    \n",
    "What about more complicated examples? (parenthesis, dashes in the middle) \n",
    "\n",
    "    Regular expressions in action \n",
    "    \n",
    "        #Replace letters with notting\n",
    "        \n",
    "            phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(r'\\D+', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad63ca2",
   "metadata": {},
   "source": [
    "#### 2.3.1 Removing titles and taking names\n",
    "\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the full_name column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f38e0667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Replace \"Dr.\" with empty string \"\"\\nairlines[\\'full_name\\'] = airlines[\\'full_name\\'].str.replace(\"Dr.\",\"\")\\n\\n# Replace \"Mr.\" with empty string \"\"\\nairlines[\\'full_name\\'] = airlines[\"full_name\"].str.replace(\"Mr.\", \"\")\\n\\n# Replace \"Miss\" with empty string \"\"\\nairlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Miss\", \"\")\\n\\n# Replace \"Ms.\" with empty string \"\"\\nairlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Ms.\", \"\")\\n\\n\\n# Assert that full_name has no honorifics\\nassert airlines[\\'full_name\\'].str.contains(\\'Ms.|Mr.|Miss|Dr.\\').any() == False\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines[\"full_name\"].str.replace(\"Mr.\", \"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Miss\", \"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Ms.\", \"\")\n",
    "\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c006b59",
   "metadata": {},
   "source": [
    "#### 2.3.2 Keeping it descriptive\n",
    "\n",
    "\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the survey_response column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new DataFrame contains responses with 40 characters or more using an assert statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines[\"survey_response\"].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849752b6",
   "metadata": {},
   "source": [
    "These types of feedbacks are essential to improving any service. Coupled with some wordcount analysis, you can find common patterns across all survey responses in no time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc68458",
   "metadata": {},
   "source": [
    "# 3. Advanced data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee8ce9",
   "metadata": {},
   "source": [
    "### 3.1 Uniformity \n",
    "\n",
    "Uniformity\n",
    "\n",
    "    For example, we can have temperature data that has values in both Fahrenheit and Celsius, weight data in Kilograms and in stones, dates in multiple formats, and so on. Verifying unit uniformity is imperative to having accurate analysis.\n",
    "    \n",
    "    \n",
    "An example \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #Create scatterplot \n",
    "    \n",
    "        plt.scatter(x = \"Date\", y = \"Temperature\", data = temperatures\n",
    "        \n",
    "    #Create title, xlabel and ylabel \n",
    "    \n",
    "        plt.title('Temperature in Celsius March 2019 - NYC')\n",
    "        plt.xlabel('Dates)\n",
    "        plt.ylabel('Temperature in Celsius')\n",
    "    \n",
    "    #Show plot\n",
    "        plt.show()\n",
    "        \n",
    "Treating temperature data\n",
    "\n",
    "    temp_fah = temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] \n",
    "    temp_cels = (temp_fah - 32) *(5/9)\n",
    "    temperatures.loc[temperatures[\"Temperature\"] > 40, \"Temperature\"] = temp_cels \n",
    "\n",
    "    #Assert conversion is correct\n",
    "    \n",
    "        assert temperatures[\"Temperature\"].max() < 40\n",
    "        \n",
    "Treating date data - different date format\n",
    "\n",
    "    Datetime formatting\n",
    "    \n",
    "        datetime is useful for representing dates \n",
    "        pandas.to_datetime()\n",
    "        \n",
    "    Standardized\n",
    "    \n",
    "        #Will work! \n",
    "        \n",
    "            birthdays[\"Birthday\"] = pd.to_datetime(birthdays[\"Birthday\"], infer_datetime_format = True, errors = \"coerce\"] \n",
    "            \n",
    "            infer - Attempt to infer format of each data\n",
    "            coerce - Return NA for rows where conversion failed \n",
    "        \n",
    "        2. Convert \n",
    "        \n",
    "        birthdays[\"Birthday\"] = birthdays[\"Birthday\"].dt.strftime(\"%d-%m-%Y\")\n",
    "        \n",
    "Treatinng ambiguous date data\n",
    "\n",
    "    Is 2019-03-08 in August or March?\n",
    "    \n",
    "        No clear cut way: \n",
    "            Convert to NA and treat accordingly \n",
    "            Infer format by understanding data source\n",
    "            Infer format by understanding previous and subsequet data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a005722",
   "metadata": {},
   "source": [
    "ambiguous dates require a thorough understanding of where your data comes from. Diagnosing problems is the first step in finding the best solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fec70e",
   "metadata": {},
   "source": [
    "#### 3.1.1 Uniform currencies \n",
    "\n",
    "The dataset contains data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66563d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Find values of acct_cur that are equal to 'euro'\\nacct_eu = banking['acct_cur'] == 'euro'\\n\\n# Convert acct_amount where it is in euro to dollars\\nbanking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\\n\\n# Unify acct_cur column by changing 'euro' values to 'dollar'\\nbanking.loc[acct_eu, 'acct_cur'] = 'dollar'\\n\\n# Assert that only dollar currency remains\\nassert banking['acct_cur'].unique() == 'dollar'\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb10ff7",
   "metadata": {},
   "source": [
    "#### 3.1.2 Uniform dates\n",
    "\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff90da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Print the header of account_opend\\nprint(banking[\\'account_opened\\'].head())\\n\\n# Convert account_opened to datetime\\nbanking[\\'account_opened\\'] = pd.to_datetime(banking[\\'account_opened\\'],\\n                                           # Infer datetime format\\n                                           infer_datetime_format = True,\\n                                           # Return missing value for error\\n                                           errors = \\'coerce\\') \\n\\n# Get year of account opened\\nbanking[\\'acct_year\\'] = banking[\\'account_opened\\'].dt.strftime(\\'%Y\\')\\n\\n# Print acct_year\\nprint(banking[\"acct_year\"])\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking[\"acct_year\"])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6eb3e",
   "metadata": {},
   "source": [
    "## 3.2 Cross field validation \n",
    "\n",
    "\n",
    "Motivation \n",
    "\n",
    "    contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight. We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.\n",
    "\n",
    "\n",
    "Cross field validation\n",
    "\n",
    "    The use of multiple fields in a dataset to santy check data integrity \n",
    "\n",
    "        sum_classes = flights[[\"economy_class\", \"business_class\", \"first_class\"]].sum(axis=1)\n",
    "        passenger_equ = sum_classes == flights[\"total_pasengers] \n",
    "    \n",
    "    Find and filter rows with inconsistent passenger totals\n",
    "        \n",
    "        inconsistent_pass = flights[~passenger_equ]\n",
    "        consistent_pass = flights[passenger_equ]\n",
    "        \n",
    "Example 2 \n",
    "\n",
    "    another example containing user IDs, birthdays and age values for a set of users. We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today's date and each birthday.\n",
    "\n",
    "    import pandas as pd\n",
    "    import datetime as dt\n",
    "    \n",
    "    #Convert to datetime and get today's date\n",
    "    \n",
    "        users[\"Birthday\"] = pd.to_datetime(users[\"Birthday\"])\n",
    "        today = dt.date.today()\n",
    "    \n",
    "    #For each row in the Birthday column, calculate year difference\n",
    "    \n",
    "        age_manual = today.year - users[\"Birthday\"].dt.year\n",
    "        \n",
    "    #Find instances where ages match\n",
    "       \n",
    "       age_equ = age_manual == users[\"Age\"]\n",
    "\n",
    "\n",
    "    # Find and filter rows with inconsistent inconsistent age \n",
    "        \n",
    "        inconsistent_pass = flights[~age_equ]\n",
    "        consistent_pass = flights[age_equ]\n",
    "        \n",
    "What to do when we catch inconsistencies\n",
    "\n",
    "    Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our dataset. \n",
    "    \n",
    "    1. We can decide to either drop inconsistent data; \n",
    "    2. set it to missing and impute it; or \n",
    "    3. apply some rules due to domain knowledge.\n",
    "    \n",
    "    All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from and the different sources feeding into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39764b",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Integrity\n",
    "\n",
    "New data has been merged into the banking DataFrame that contains details on how investments in the inv_amount column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the age and birth_date columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' birthdays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3665f714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Store fund columns to sum against\\nfund_columns = banking[[\\'fund_A\\', \\'fund_B\\', \\'fund_C\\', \\'fund_D\\']]\\n\\n# Find rows where fund_columns row sum == inv_amount\\ninv_equ = fund_columns.sum(axis=1) == banking[\\'inv_amount\\']\\n\\n# Store consistent and inconsistent data\\nconsistent_inv = banking[inv_equ]\\ninconsistent_inv = banking[~inv_equ]\\n\\n# Store consistent and inconsistent data\\nprint(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Store fund columns to sum against\n",
    "fund_columns = banking[['fund_A', 'fund_B', 'fund_C', 'fund_D']]\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = fund_columns.sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1982314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Store today\\'s date and find ages\\ntoday = dt.date.today()\\nages_manual = today.year - banking[\"birth_date\"].dt.year\\n\\n# Find rows where age column == ages_manual\\nage_equ = ages_manual == banking[\"age\"]\\n\\n# Store consistent and inconsistent data\\nconsistent_ages = banking[age_equ]\\ninconsistent_ages = banking[~age_equ]\\n\\n# Store consistent and inconsistent data\\nprint(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking[\"birth_date\"].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking[\"age\"]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc28761",
   "metadata": {},
   "source": [
    "## 3.3 Completeness\n",
    "\n",
    "\n",
    "What is missing data?\n",
    "\n",
    "    Missing data is one of the most common and most important data cleaning problems. Essentially, missing data is when no data value is stored for a variable in an observation. Missing data is most commonly represented as NA or NaN, but can take on arbitrary values like 0 or dot.\n",
    "\n",
    "Airquality example\n",
    "\n",
    "    #return missing values\n",
    "    \n",
    "        airquality.isna()\n",
    "   \n",
    "    #Get summary of missing ness\n",
    "        \n",
    "        airquality.isna().sum()\n",
    "\n",
    "Missingno\n",
    "    \n",
    "    create useful visualizations of our missing data.\n",
    "    \n",
    "        import missingno as msno \n",
    "        import matplotlib.pyplot\n",
    "    \n",
    "    #Visualize missingness\n",
    "        msno.matrix(airquality)\n",
    "        plt.show()\n",
    "        \n",
    "    #Isolate missing and complete values aside\n",
    "    \n",
    "        missing = airquality[airquality[\"C02\"].isna()]\n",
    "        complete = airquality[~airquality[\"C02\"].isna()]\n",
    "        \n",
    "    #Describe complete and missing dataframe\n",
    "        complete.describe()\n",
    "        missing.describe()\n",
    "        \n",
    "    #\n",
    "    sorted_airquality = airquality.sort_values(by = \"Temperature\")\n",
    "    msno,matrix(sorted_airquality)\n",
    "    plt.show()\n",
    "    \n",
    "Missingness types  \n",
    "\n",
    "    1. Missing Completely at Random (MCAR)\n",
    "        \n",
    "        No systematic relationship between missing data and other values\n",
    "        Data entry errors when inputting data\n",
    "    \n",
    "    2. Missing at Random (MAR)\n",
    "        \n",
    "        Systematic relationship between missing data and observed values\n",
    "        Missing ozone data for high temp \n",
    "        \n",
    "    3. Missing not at random (MNAR)\n",
    "    \n",
    "        Systematic relationship between the missing data and unobserved values\n",
    "        Missing temperature values for high temp \n",
    "    \n",
    "    \n",
    "How to deal with missing data?\n",
    "\n",
    "Simple approaches\n",
    "\n",
    "    1. Drop missing values\n",
    "        \n",
    "        airquality_dropped = airquality.dropna(subset = [\"C02\"]) \n",
    "        airquality_dropped.head()\n",
    "     \n",
    "    2. Impute with statistical measures (mean, median, mode)\n",
    "    \n",
    "        co2_mean = airquality['C02\"].mean()\n",
    "        airquality_imputed = airquality.fillna({\"C02\":co2_mean})\n",
    "        airquality_imputed.head()\n",
    "\n",
    "More complex\n",
    "\n",
    "    1. Impute using an algorithmic approach\n",
    "    2. Impute with machine learning models \n",
    "    \n",
    "    \n",
    "TRY!!! You have a dataframe containing customer satisfaction scores for a service. What ype of missing is the following? \n",
    "    \n",
    "    A customer satisfaction_score column with missing values for highly dissatisfied \n",
    "    \n",
    "    This is a clear example of missing not at random, where low values of satisfaction_score are missing because of inherently low satisfaction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c71f6",
   "metadata": {},
   "source": [
    "#### 3.3.1 Missing investors \n",
    "\n",
    "You just received a new version of the banking DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing inv_amount values.\n",
    "\n",
    "You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4514b5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Print number of missing values in banking\\nprint(banking.isna().sum())\\n\\n# Visualize missingness matrix\\nmsno.matrix(banking)\\nplt.show()\\n\\n# Isolate missing and non missing values of inv_amount\\nmissing_investors = banking[banking[\\'inv_amount\\'].isna()]\\ninvestors = banking[~banking[\\'inv_amount\\'].isna()]\\n\\n# Sort banking by age and visualize\\nbanking_sorted = banking.sort_values(by = \"age\")\\nmsno.matrix(banking_sorted)\\nplt.show()\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = \"age\")\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1551b",
   "metadata": {},
   "source": [
    "Great job! Notice how all the white spaces for inv_amount are on top? Indeed missing values are only due to young bank account holders not investing their money! Better set it to 0 with .fillna()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa40d78",
   "metadata": {},
   "source": [
    "#### 3.3.2 Follow the money\n",
    "\n",
    "In this exercise, you're working with another version of the banking DataFrame that contains missing values for both the cust_id column and the acct_amount column.\n",
    "\n",
    "You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing cust_id don't really help you, and that on average acct_amount is usually 5 times the amount of inv_amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3549197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Drop missing values of cust_id\\nbanking_fullid = banking.dropna(subset = [\\'cust_id\\'])\\n\\n# Compute estimated acct_amount\\nacct_imp = banking_fullid[\"inv_amount\"] * 5\\n\\n# Impute missing acct_amount with corresponding acct_imp\\nbanking_imputed = banking_fullid.fillna({\\'acct_amount\\':acct_imp})\\n\\n# Print number of missing values\\nprint(banking_imputed.isna().sum())\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid[\"inv_amount\"] * 5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba0fb2",
   "metadata": {},
   "source": [
    "# 4. Record Linkages \n",
    "\n",
    "    Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset.\n",
    "\n",
    "\n",
    "Minimum edit distance\n",
    "    \n",
    "    systematic way to identify how close 2 strings are\n",
    "    \n",
    "    For example, let's take a look at the following two words: intention, and execution. The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being inserting new characters, deleting them, substituting them, and transposing consecutive characters.\n",
    "\n",
    "Example\n",
    "\n",
    "    To get from intention to execution, We first start off by deleting I from intention, and adding C between E and N. Our minimum edit distance so far is 2, since these are two operations. Then we substitute the first N with E, T with X, and N with U, leading us to execution! With the minimum edit distance being 5.\n",
    "    \n",
    "    The lower the edit distance, the closer two words are. \n",
    "    \n",
    "Minimum edit distance algorithms\n",
    "\n",
    "    There's a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they're suited for and more, with a variety of packages to get each similarity.\n",
    "    \n",
    "    For this lesson, we'll be comparing strings using Levenshtein distance since it's the most general form of string matching by using the thefuzz package.\n",
    "\n",
    "Simple string comparison \n",
    "\n",
    "    thefuzz is a package to perform string comparison. We first import fuzz from thefuzz, which allow us to compare between single strings. \n",
    "    \n",
    "        from thefuzz import fuzz\n",
    "    \n",
    "    Here we use fuzz's WRatio function to compute the similarity between reading and its typo, inputting each string as an argument. \n",
    "    \n",
    "        fuzz.WRatio('Reeding', Reading') - 86\n",
    "        \n",
    "    For any comparison function using thefuzz, our output is a score from 0 to 100 with 0 being not similar at all, 100 being an exact match. Do not confuse this with the minimum edit distance score from earlier, where a lower minimum edit distance means a closer match.\n",
    "    \n",
    "Partial strings and different orderings   \n",
    "\n",
    "    WRatio function is highly robust against partial string comparison with different orderings. For example here we compare the strings Houston Rockets and Rockets, and still receive a high similarity score. The same can be said for the strings Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets, where the team names are only partial and they are differently ordered.\n",
    "    \n",
    "Comparison with arrays    \n",
    "\n",
    "    #Import\n",
    "    \n",
    "        from thefuzz import process\n",
    "    \n",
    "    #Define string and array\n",
    "    \n",
    "        string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "        choices = pd.Series([\"Rockets vs Lakers\", \"Lakers vs Rockets\", \n",
    "                              'Houston vs Los Angeles', 'Heat vs Bulls'])\n",
    "                              \n",
    "        process.extract(string, choices limit = 2)\n",
    "        \n",
    "     Extract takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest. \n",
    "     \n",
    "     It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array.\n",
    "     \n",
    "Collapsing categories with string similarity\n",
    "\n",
    "    collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!\n",
    "    \n",
    "    \n",
    "Example\n",
    "\n",
    "Say we have DataFrame named survey containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we'll use string similarity. We also have a category DataFrame containing the correct categories for each state. Let's collapse the incorrect categories with string matching!\n",
    "\n",
    "    print(survey[\"state\"].unique()) \n",
    "\n",
    "Collapsing all of the state\n",
    "\n",
    "#For each correct category\n",
    "\n",
    "We first create a for loop iterating over each correctly typed state in the categories DataFrame. \n",
    "\n",
    "    for state in categories['state']\n",
    "    \n",
    "    #Find potential matches in states with typoes\n",
    "    \n",
    "        matches = process.extract(state, survey[\"state'], limit = survey.shape[0])\n",
    "    \n",
    "        For each state, we find its matches in the state column of the survey DataFrame, returning all possible matches by setting the limit argument of extract to the length of the survey DataFrame. \n",
    "\n",
    "            #For each potential match \n",
    "            \n",
    "            #we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement.\n",
    "            \n",
    "                for potential_match in matches: \n",
    "                \n",
    "                #if high similarity score\n",
    "                    if potential_match[1] >= 80: \n",
    "                    \n",
    "                    #replace typo with correct category\n",
    "                    #Then for each of those returned strings, we replace it with the correct state using the loc method.\n",
    "                    \n",
    "                        survey.loc[survey[\"state\"] == potential_match[0], 'state'] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c47ef",
   "metadata": {},
   "source": [
    "### 4.1 The cutoff point\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the cuisine_type column is riddled with typos, and should contain only italian, american and asian cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the thefuzz's process.extract() function by finding the similarity score of the most distant typo of each category.\n",
    "\n",
    "    The limit argument should take the length of the unique_types array.\n",
    "    process.extract() takes in a string, an array to compare against, and the number of matches to return in the limit argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f43841",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    [('asian', 100), ('asiane', 91), ('asiann', 91), ('asiian', 91), ('asiaan', 91), ('asianne', 83), ('asiat', 80), ('italiann', 72), ('italiano', 72), ('italianne', 72), ('italian', 67), ('amurican', 62), ('american', 62), ('italiaan', 62), ('italiian', 62), ('itallian', 62), ('americann', 57), ('americano', 57), ('ameerican', 57), ('aamerican', 57), ('ameriican', 57), ('amerrican', 57), ('ammericann', 54), ('ameerrican', 54), ('ammereican', 54), ('america', 50), ('merican', 50), ('murican', 50), ('italien', 50), ('americen', 46), ('americin', 46), ('amerycan', 46), ('itali', 40)]\n",
    "    [('american', 100), ('americann', 94), ('americano', 94), ('ameerican', 94), ('aamerican', 94), ('ameriican', 94), ('amerrican', 94), ('america', 93), ('merican', 93), ('ammericann', 89), ('ameerrican', 89), ('ammereican', 89), ('amurican', 88), ('americen', 88), ('americin', 88), ('amerycan', 88), ('murican', 80), ('asian', 62), ('asiane', 57), ('asiann', 57), ('asiian', 57), ('asiaan', 57), ('italian', 53), ('asianne', 53), ('italiann', 50), ('italiano', 50), ('italiaan', 50), ('italiian', 50), ('itallian', 50), ('italianne', 47), ('asiat', 46), ('itali', 40), ('italien', 40)]\n",
    "    [('italian', 100), ('italiann', 93), ('italiano', 93), ('italiaan', 93), ('italiian', 93), ('itallian', 93), ('italianne', 88), ('italien', 86), ('itali', 83), ('asian', 67), ('asiane', 62), ('asiann', 62), ('asiian', 62), ('asiaan', 62), ('asianne', 57), ('amurican', 53), ('american', 53), ('americann', 50), ('asiat', 50), ('americano', 50), ('ameerican', 50), ('aamerican', 50), ('ameriican', 50), ('amerrican', 50), ('ammericann', 47), ('ameerrican', 47), ('ammereican', 47), ('america', 43), ('merican', 43), ('murican', 43), ('americen', 40), ('americin', 40), ('amerycan', 40)]\n",
    "    \n",
    "    \n",
    "    Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?\n",
    "    \n",
    "    \n",
    "    Correct! 80 is that sweet spot where you convert all incorrect typos without remapping incorrect categories. Often times though, you may need to combine the techniques learned in chapter 2, especially since there could be strings that make it beyond our cutoff point, but are not actually a match!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da9fc1",
   "metadata": {},
   "source": [
    "### 4.2 Remapping categories II\n",
    "\n",
    "In the last exercise, you determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian' cuisine types stored in the cuisine_type column should be 80.\n",
    "\n",
    "In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using process.extract(), the output is a list of tuples where each is formatted like:\n",
    "\n",
    "(closest match, similarity score, index of match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Return all of the unique values in the cuisine_type column of restaurants.\n",
    "\n",
    "    print(restaurants[\"cuisine_type\"].unique())\n",
    "\n",
    "# As a first step, create a list of all possible matches, \n",
    "# comparing 'italian' with the restaurant types listed in the cuisine_type column.\n",
    "\n",
    "    matches = process.extract('italian', restaurants['cuisine_type'], limit = restaurants.shape[0])\n",
    "\n",
    "    # Inspect the first 5 matches\n",
    "    print(matches[0:5])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac378f20",
   "metadata": {},
   "source": [
    "The score is the second element of your match variable (remember that Python is zero-indexed).\n",
    "\n",
    "You can use .loc[] combined with a condition to reassign the matches that are similar to 'italian' to 'italian' using the following syntax: df.loc[df['column'] == old_value] = new_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37afb005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Create a list of matches, comparing 'italian' with the cuisine_type column\\nmatches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\\n\\n# Iterate through the list of matches to italian\\nfor match in matches:\\n  # Check whether the similarity score is greater than or equal to 80\\n  if match[1] >= 80:\\n    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\\n    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ef4fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Iterate through categories\\nfor cuisine in categories:  \\n\\n  # Create a list of matches, comparing cuisine with the cuisine_type column\\n  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\\n\\n  # Iterate through the list of matches\\n  \\n  for match in matches:\\n  \\n     # Check whether the similarity score is greater than or equal to 80\\n     \\n    if match[1] >= 80:\\n    \\n     # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\\n      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\\n      \\n# Inspect the final result\\n\\nprint(restaurants['cuisine_type'].unique())\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "\n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  \n",
    "  for match in matches:\n",
    "  \n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "     \n",
    "    if match[1] >= 80:\n",
    "    \n",
    "     # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "\n",
    "print(restaurants['cuisine_type'].unique())\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0e949",
   "metadata": {},
   "source": [
    "### 4.3 Generating pairs\n",
    "\n",
    "Motivation\n",
    "\n",
    "    At the end of the last video exercise, we saw how record linkage attempts to join data sources with fuzzy duplicate values. For example here are two DataFrames containing NBA games and their schedules. They've both been scraped from different sites and we would want to merge them together and have one DataFrame containing all unique games.\n",
    "    \n",
    "When joins won't work\n",
    "\n",
    "    We see that there are duplicates values in both DataFrames with different naming marked here in red, and non duplicate values, marked here in green. Since there are games happening at the same time, no common unique identifier between the DataFrames, and the events are differently named, a regular join or merge will not work. This is where record linkage comes in.\n",
    "\n",
    "Record linkage\n",
    "\n",
    "    act of linking data from different sources regarding the same entity. Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them.\n",
    "    \n",
    "       1. Clean two dataset\n",
    "       2. generate pairs\n",
    "       3. Compare pairs\n",
    "       4. Score pairs\n",
    "       5. Link data \n",
    "\n",
    "Our DataFrames\n",
    "\n",
    "    Ideally, we want to generate all possible pairs between our DataFrames. but what if we had big DataFrames and ended up having to generate millions if not billions of pairs? It wouldn't prove scalable and could seriously hamper development time.\n",
    "\n",
    "    Generating pairs\n",
    "    \n",
    "    This is where we apply what we call blocking, which creates pairs based on a matching column, which is in this case, the state column, reducing the number of possible pairs.\n",
    "    \n",
    "        A. Blocking\n",
    "        \n",
    "            import recordlinkage\n",
    "            \n",
    "            indexer = recordlinkage.index()\n",
    "            \n",
    "            #Generate pairs blocked on state\n",
    "            \n",
    "                indexer.block(\"state\")\n",
    "                pairs = indexer.index(census_A, census_B)\n",
    "                \n",
    "                The resulting object, is a pandas multi index object containing pairs of row indices from both DataFrames, which is a fancy way to say it is an array containing possible pairs of indices that makes it much easier to subset DataFrames on.\n",
    "\n",
    "Comparing the DataFrames\n",
    "\n",
    "    We then use the recordlinkage dot Index function, to create an indexing object. This essentially is an object we can use to generate pairs from our DataFrames. To generate pairs blocked on state, we use the block method, inputting the state column as input. Once the indexer object has been initialized, we generate our pairs using the dot index method, which takes in the two dataframes.\n",
    "\n",
    "    #Generate pairs blocked on state\n",
    "            \n",
    "         pairs = indexer.Index(census_A, census_B)\n",
    "         \n",
    "    Since we've already generated our pairs, it's time to find potential matches. We first start by creating a comparison object using the recordlinkage dot compare function. This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs. \n",
    "         \n",
    "    #Create a Compare object \n",
    "    \n",
    "        compare_cl = recordlinkage.Compare()\n",
    "        \n",
    "    #Find exact matches for pairs of date of birth and state\n",
    "    \n",
    "        compare_cl.exact(\"date of birth\", \"date of birth\", label = \"date of birth\")\n",
    "        compare_cl.exact(\"state\", \"state\", label = \"state\")\n",
    "        \n",
    "    # Find similar matches for pairs of surname and address_1 ysing string similarity\n",
    "    \n",
    "    Now in order to compute string similarities between pairs of rows for columns that have fuzzy values, we use the dot string method, which also takes in the column names in question, the similarity cutoff point in the threshold argument, which takes in a value between 0 and 1, which we here set to 0.85. \n",
    "    \n",
    "        compare_cl.string(\"surname\", \"surname\", threshold = 0.85, label = \"surname\")\n",
    "        compare_cl.string(\"address_1\", \"address_1\", threshold = 0.85, label = \"address_1\")\n",
    "    \n",
    "    #Find matches \n",
    "    \n",
    "        potential_matches = compare_cl.compute(pairs, census_A, census_B)\n",
    "        print(potential_matches) \n",
    "    \n",
    "   \n",
    "    #Find only pairs we want \n",
    "    \n",
    "        potential_matches[potential_matches.sum(axis = 1) >=2]\n",
    "        \n",
    "        To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. Which in this case higher or equal to 2. But we'll dig deeper into these matches and see how to use them to link our census DataFrames in the next lesson.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869873e",
   "metadata": {},
   "source": [
    "Similar to joins, record linkage is the act of linking data from different sources regarding the same entity. But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely upon when linking data sources such as a unique identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30ca48",
   "metadata": {},
   "source": [
    "#### 4.3.1 PRACTICE\n",
    "\n",
    "In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named restaurants_new with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "\n",
    "You've already cleaned the cuisine_type and city columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names that require record linkage instead of joins with restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052bb155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Create an indexer and object and find possible pairs\\nindexer = recordlinkage.Index()\\n\\n# Block pairing on cuisine_type\\nindexer.block('cuisine_type')\\n\\n# Generate pairs\\npairs = indexer.index(restaurants, restaurants_new)\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddb15e",
   "metadata": {},
   "source": [
    "#### Similar restaurants\n",
    "\n",
    "\n",
    "In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames using record linkage.\n",
    "\n",
    "When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and cuisine_type columns between each pair, and similar strings for each pair in the rest_name column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53a29a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create a comparison object\\ncomp_cl = recordlinkage.Compare()\\n\\n# Find exact matches on city, cuisine_types - \\ncomp_cl.exact('city', 'city', label='city')\\ncomp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\\n\\n# Find similar matches of rest_name\\ncomp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \\n\\n# Get potential matches and print\\npotential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\\nprint(potential_matches)\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types - \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc8f67",
   "metadata": {},
   "source": [
    "Print out potential_matches, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with\n",
    "\n",
    "    potential_matches[potential_matches.sum(axis = 1) >= n]\n",
    "    \n",
    "Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of n be?\n",
    "\n",
    "    WRONG!!! n = 2 - If n is set to 2, then you will get duplicates for all restaurants with the same cuisine type in the same city.\n",
    "    \n",
    "    \n",
    "    WRONG! n = 1 - What if you had restaurants with the same name in different cities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7686f",
   "metadata": {},
   "source": [
    "### 4.4 Linking DataFrames\n",
    "\n",
    "Probable matches\n",
    "\n",
    "    The first step in linking DataFrames, is to isolate the potentially matching pairs to the ones we're pretty sure of. We saw how to do this in the previous lesson, by subsetting the rows where the row sum is above a certain number of columns, in this case 3. The output is row indices between census A and census B that are most likely duplicates.  \n",
    "    \n",
    "    next step is to extract the one of the index columns, and subsetting its associated DataFrame to filter for duplicates.\n",
    "    \n",
    "    Here we choose the second index column, which represents row indices of census B. We want to extract those indices, and subset census_B on them to remove duplicates with census_A before appending them together.\n",
    "    \n",
    "Get the indices\n",
    "\n",
    "    We can access a DataFrame's index using the index attribute. Since this is a multi index DataFrame, it returns a multi index object containing pairs of row indices from census_A and census_B respectively. We want to extract all census_B indices, so we chain it with the get_level_values method, which takes in which column index we want to extract its values. We can either input the index column's name, or its order, which is in this case 1.\n",
    "    \n",
    "    \n",
    "    #Find matches \n",
    "    \n",
    "        potential_matches = compare_cl.compute(full_pairs, census_A, census_B)\n",
    "    \n",
    "   \n",
    "    #Find only pairs we want \n",
    "    \n",
    "        matches = potential_matches[potential_matches.sum(axis = 1) >=3]\n",
    "    \n",
    "    #Get indices \n",
    "    matches.index\n",
    "    \n",
    "    #Get indices from census_B only\n",
    "    \n",
    "        duplicate_rows = matches.index.get_level_values(1)\n",
    "        print(census_B_index)\n",
    "    \n",
    "    #Finding duplicates in census B\n",
    "    \n",
    "        census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
    "    \n",
    "    #Finidng new rows \n",
    "    census_B_duplicates = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "    \n",
    "    #Link dataframes\n",
    "    \n",
    "        full_census = census_A.append(census_B_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613d08e",
   "metadata": {},
   "source": [
    "#### Linking them together!\n",
    "\n",
    "In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. You've generated the different pairs of potentially matching rows, searched for exact matches between the cuisine_type and city columns, but compared for similar strings in the rest_name column. You stored the DataFrame containing the scores in potential_matches.\n",
    "\n",
    "Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of restaurants_new that are matching across the columns mentioned above from potential_matches. Then you will subset restaurants_new on these indices, then append the non-duplicate values to restaurants\n",
    "\n",
    "\n",
    "Instructions: \n",
    "\n",
    "    Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
    "\n",
    "    Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.\n",
    "\n",
    "    Subset restaurants_new for rows that are not in matching_indices.\n",
    "\n",
    "    Append non_dup to restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7934654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Isolate potential matches with row sum >=3\\nmatches = potential_matches[potential_matches.sum(axis =1) >= 3]\\n\\n# Get values of second column index of matches\\nmatching_indices = matches.index.get_level_values(1)\\n\\n# Subset restaurants_new based on non-duplicate values\\nnon_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\\n\\n# Append non_dup to restaurants\\nfull_restaurants = restaurants.append(non_dup)\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis =1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
