{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23374d32",
   "metadata": {},
   "source": [
    "# 1. DataFrames\n",
    "\n",
    "What is the Point of Pandas?\n",
    "\n",
    "    1. Data Manipulation\n",
    "    2. Data Visualization \n",
    "    \n",
    "Pandas is built on NumPy and Matplotlib\n",
    "\n",
    "    Numpy: provides multidimensional array objects for easy data manipulation that pandas uses to store data \n",
    "    Matplotlib: data visualization capabilities that pandas takes advantage of \n",
    "    \n",
    "    Exploring a DataFrame: \n",
    "        .head() - return first few rows \n",
    "        .info() - names of columns, the data types they contain, and whether they have any missing values.\n",
    "        .shap - contains a tuple that holds the number of rows followed by the number of column (row x column)\n",
    "        .describe() - summ stat\n",
    "        \n",
    "    Components of a DataFrame: \n",
    "        .values - contains the data values in a 2-dimensional NumPy array.\n",
    "        .columns - contains column names\n",
    "        .index - contains row numbers or row names. Be careful, since row labels are stored in dot-index, not in dot-rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e37b2",
   "metadata": {},
   "source": [
    "### 1.1 Inspecting a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbb89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print the head of the homelessness data\\nprint(homelessness.head())\\n\\n# Print information about homelessness\\nprint(homelessness.info())\\n\\n# Print the shape of homelessness\\nprint(homelessness.shape)\\n\\n# Print a description of homelessness\\nprint(homelessness.describe())'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "print(homelessness.describe())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1cd66",
   "metadata": {},
   "source": [
    "### 1.2 Parts of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66d9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import pandas using the alias pd\\nimport pandas as pd \\n\\n# Print the values of homelessness\\nprint(homelessness.values)\\n\\n# Print the column index of homelessness\\nprint(homelessness.columns)\\n\\n\\n# Print the row index of homelessness\\nprint(homelessness.index)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Import pandas using the alias pd\n",
    "import pandas as pd \n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51366044",
   "metadata": {},
   "source": [
    "### 1.3 Sorting and Subsetting\n",
    "\n",
    "Sorting: dogs.sort_values(\"weight_kg\")\n",
    "\n",
    "Sorting in descending order: dogs.sort_values(\"weight_kg\", ascending = False)\n",
    "\n",
    "Sorting by multiple variables: dogs.sort_values([\"weight_kg\", \"height_cm\"], ascending = [True,False])\n",
    "\n",
    "Subsetting columns: dogs[\"name\"]\n",
    "\n",
    "Subsetting multiple columns: dogs[[\"breed\", \"height_cm]]\n",
    "\n",
    "    the inner and outer square brackets are performing different tasks. The outer square brackets are responsible for               subsetting the DataFrame, and the inner square brackets are creating a list of column names to subset. \n",
    "\n",
    "Subsetting rows: dogs[dogs[\"height_cm\"] > 50]\n",
    "\n",
    "Subsetting based on text data: dogs[dogs[\"breed\"] > \"Labrador]\n",
    "\n",
    "Subsetting based on dates: dogs[dogs[\"date_of_birth\"] < \"2015-01-01\"]\n",
    "\n",
    "Subsetting based on multiple conditions: \n",
    "\n",
    "    is_lab = dogs[\"breed\"] = \"Labrador\"\n",
    "    is_brown = dogs[\"color\"] = \"Brown\"\n",
    "    dogs[is_lab & is_brown]\n",
    "\n",
    "    \n",
    "Subsetting using .isin(): \n",
    "\n",
    "\n",
    "    is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    dogs[is_black_or_brown] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8db5a",
   "metadata": {},
   "source": [
    "#### 1.3.1 Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f2fd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sort homelessness by individuals\\nhomelessness_ind = homelessness.sort_values(\"individuals\")\\n\\n# Print the top few rows\\nprint(homelessness_ind.head())'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_ind.head())''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b10bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sort homelessness by descending family members\\nhomelessness_fam = homelessness.sort_values(\"family_members\", ascending = False)\\n\\n# Print the top few rows\\nprint(homelessness_fam.head())'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending = False)\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29612d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sort homelessness by region, then descending family members\\nhomelessness_reg_fam = homelessness.sort_values([\"region\",\"family_members\"] , ascending = [True, False])\\n\\n# Print the top few rows\\nprint(homelessness_reg_fam.head())'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values([\"region\",\"family_members\"] , ascending = [True, False])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_reg_fam.head())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e757f79",
   "metadata": {},
   "source": [
    "#### 1.3.2 Subsetting Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e6db56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Select the individuals column\\nindividuals = homelessness[\"individuals\"]\\n\\n# Print the head of the result\\nprint(individuals.head())'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the head of the result\n",
    "print(individuals.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30269d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Select the state and family_members columns\\nstate_fam = homelessness[[\"state\", \\'family_members\\']]\\n\\n# Print the head of the result\\nprint(state_fam.head())'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", 'family_members']]\n",
    "\n",
    "# Print the head of the result\n",
    "print(state_fam.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95e0bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Select only the individuals and state columns, in that order\\nind_state = homelessness[[\"individuals\", \"state\"]]\\n\\n# Print the head of the result\\nprint(ind_state.head())'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f891dad",
   "metadata": {},
   "source": [
    "#### 1.3.3 Subsetting Rows\n",
    "\n",
    "    dogs[dogs[\"height_cm\"] > 60]\n",
    "    dogs[dogs[\"color\"] == \"tan\"]\n",
    "    \n",
    "    dogs[(dogs[\"height_cm\"] > 60) & (dogs[\"color\"] == \"tan\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d52389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Filter for rows where individuals is greater than 10000\\nind_gt_10k = homelessness[homelessness[\"individuals\"] > 1000]\\n\\n# See the result\\nprint(ind_gt_10k)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 1000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abae351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Filter for rows where region is Mountain\\nmountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\\n\\n# See the result\\nprint(mountain_reg)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b19243d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Filter for rows where family_members is less than 1000 \\n# and region is Pacific\\nfam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\\n\\n# See the result\\nprint(fam_lt_1k_pac)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23910787",
   "metadata": {},
   "source": [
    "#### 1.3.4 Subsetting rows by categorical variables\n",
    "\n",
    "    colors = [\"brown\", \"black\", \"tan\"]\n",
    "    condition = dogs[\"color\"].isin(colors)\n",
    "    dogs[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c58da97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSubset for rows in South Atlantic or Mid-Atlantic regions\\nsouth_mid_atlantic = homelessness[homelessness[\"region\"].isin([\"South Atlantic\",\"Mid-Atlantic\"])]\\n\\n# See the result\\nprint(south_mid_atlantic)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[homelessness[\"region\"].isin([\"South Atlantic\",\"Mid-Atlantic\"])]\n",
    "\n",
    "# See the result\n",
    "print(south_mid_atlantic)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "029c9b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The Mojave Desert states\\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\\n\\n# Filter for rows in the Mojave Desert states\\nmojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\\n\\n# See the result\\nprint(mojave_homelessness)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118cade",
   "metadata": {},
   "source": [
    "### 1.4 New columns\n",
    "\n",
    "    You can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\n",
    "\n",
    "    You can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding \n",
    "    columns together or by changing their units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613664f0",
   "metadata": {},
   "source": [
    "#### 1.4.1 Adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44abdf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Add total col as sum of individuals and family_members\\nhomelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\\n\\n# Add p_individuals col as proportion of total that are individuals\\nhomelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"] \\n\\n# See the result\\nprint(homelessness)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Add total col as sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_individuals col as proportion of total that are individuals\n",
    "homelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"] \n",
    "\n",
    "# See the result\n",
    "print(homelessness)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c0032be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create indiv_per_10k col as homeless individuals per 10k state pop\\nhomelessness[\"indiv_per_10k\"] = 10000 * ____ / ____ \\n\\n# Subset rows for indiv_per_10k greater than 20\\nhigh_homelessness = ____\\n\\n# Sort high_homelessness by descending indiv_per_10k\\nhigh_homelessness_srt = ____\\n\\n# From high_homelessness_srt, select the state and indiv_per_10k cols\\nresult = ____\\n\\n# See the result\\nprint(result)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * ____ / ____ \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = ____\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = ____\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = ____\n",
    "\n",
    "# See the result\n",
    "print(result)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88c5c0",
   "metadata": {},
   "source": [
    "####  1.4.2 Combo-attack!\n",
    "\n",
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\n",
    "\n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57e465a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Create indiv_per_10k col as homeless individuals per 10k state pop\\nhomelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\\n\\n# Subset rows for indiv_per_10k greater than 20\\nhigh_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\\n\\n# Sort high_homelessness by descending indiv_per_10k\\nhigh_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending = False)\\n\\n# From high_homelessness_srt, select the state and indiv_per_10k cols\\nresult = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\\n\\n# See the result\\nprint(result)\\nprint(homelessness)\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending = False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "print(homelessness)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda6f06",
   "metadata": {},
   "source": [
    "Cool combination! District of Columbia has the highest number of homeless individuals - almost 54 per ten thousand people. This is almost double the number of the next-highest state, Hawaii. If you combine new column addition, row subsetting, sorting, and column selection, you can answer lots of questions like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eac643",
   "metadata": {},
   "source": [
    "# 2. Aggregating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2194211",
   "metadata": {},
   "source": [
    "### 2.1 Summary Statistics\n",
    "\n",
    "    Summarizing numerical data: dogs[\"height_cm\"].mean() \n",
    "      .median(), .mode(), .min(), .max(), .var(), .std()\n",
    "      .sum(), .cumsum()\n",
    "      \n",
    "    The agg method \n",
    "    \n",
    "    def pct30(column)\n",
    "        return column.quantile(0.3)\n",
    "        \n",
    "    call function - dogs[\"weight_kg\"].agg(pct30)\n",
    "    \n",
    "    Summaries on multiple columns\n",
    "        dogs[[\"weight_kg\", \"height_cm\"]].agg(pct30)\n",
    "    \n",
    "      \n",
    "    def pct40(column)\n",
    "        return column.quantile(0.4) \n",
    "    \n",
    "    dogs[[\"weight_kg\", \"height_cm\"]].agg([pct30,pct40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870d961",
   "metadata": {},
   "source": [
    "#### 2.1.1 Mean and Median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f172f8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print the head of the sales DataFrame\\nprint(sales.head())\\n\\n# Print the info about the sales DataFrame\\nprint(sales.info())\\n\\n# Print the mean of weekly_sales\\nprint(sales[\"weekly_sales\"].mean())\\n\\n# Print the median of weekly_sales\\nprint(sales[\"weekly_sales\"].median())'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18377a41",
   "metadata": {},
   "source": [
    "The mean weekly sales amount is almost double the median weekly sales amount! This can tell you that there are a few very high sales weeks that are making the mean so much higher than the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd4e73",
   "metadata": {},
   "source": [
    "#### 2.1.2 Summarizing dates\n",
    "\n",
    "Summary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4158bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print the maximum of the date column\\nprint(sales[\"date\"].max())\\n\\n# Print the minimum of the date column\\nprint(sales[\"date\"].min())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c763e",
   "metadata": {},
   "source": [
    "Taking the minimum and maximum of a column of dates is handy for figuring out what time period your data covers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb41c7",
   "metadata": {},
   "source": [
    "#### 2.1.3 Efficient summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bb87d",
   "metadata": {},
   "source": [
    "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,\n",
    "\n",
    "    # A custom IQR function\n",
    "    def iqr(column):\n",
    "        return column.quantile(0.75) - column.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab25c327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# A custom IQR function\\ndef iqr(column):\\n    return column.quantile(0.75) - column.quantile(0.25)\\n    \\n# Print IQR of the temperature_c column\\nprint(sales[\"temperature_c\"].agg(iqr))'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39348329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# A custom IQR function\\ndef iqr(column):\\n    return column.quantile(0.75) - column.quantile(0.25)\\n\\n# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\\nprint(sales[[\"temperature_c\", \\'fuel_price_usd_per_l\\', \\'unemployment\\']].agg(iqr))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", 'fuel_price_usd_per_l', 'unemployment']].agg(iqr))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f6da48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Import NumPy and create custom IQR function\\nimport numpy as np\\ndef iqr(column):\\n    return column.quantile(0.75) - column.quantile(0.25)\\n\\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe983db",
   "metadata": {},
   "source": [
    " The .agg() method makes it easy to compute multiple statistics on multiple columns, all in just one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dff05",
   "metadata": {},
   "source": [
    "#### 2.1.4 Cumulative Statistics\n",
    "\n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2f8fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Sort sales_1_1 by date\\nsales_1_1 = sales_1_1.sort_values(\"date\")\\n\\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\\nsales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\\n\\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\\nsales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\\n\\n# See the columns you calculated\\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(\"date\")\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516f083",
   "metadata": {},
   "source": [
    "### 2.2 Counting \n",
    "\n",
    "    Drop duplicates\n",
    "   \n",
    "        vet_visits.drop_duplicates(subset = \"name\") \n",
    "\n",
    "    Drop Duplicate pairs\n",
    "    \n",
    "        vet_visits.drop_duplicate(subset = [\"name\", \"breed\"]) \n",
    "        \n",
    "        \n",
    "    Count instances \n",
    "    \n",
    "           unique_dogs[\"breed\"].value_counts()\n",
    "           unique_dogs[\"breed\"].value_counts(sort = True)\n",
    "           \n",
    "    Proportions\n",
    "           unique_dogs[\"breed\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed637d",
   "metadata": {},
   "source": [
    "#### 2.2.1 Dropping duplicates\n",
    "\n",
    "Removing duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3636e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Drop duplicate store/type combinations\\nstore_types = sales.drop_duplicates(subset = [\"store\", \"type\"])\\nprint(store_types.head())\\n\\n# Drop duplicate store/department combinations\\nstore_depts =sales.drop_duplicates(subset = [\"store\", \"department\"])\\nprint(store_depts.head())\\n\\n# Subset the rows where is_holiday is True and drop duplicate dates\\nholiday_dates = sales[sales[\"is_holiday\"]==True].drop_duplicates(subset = \"date\")\\n\\n# Print date col of holiday_dates\\nprint(holiday_dates[\"date\"])\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset = [\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts =sales.drop_duplicates(subset = [\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]==True].drop_duplicates(subset = \"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates[\"date\"])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11576b",
   "metadata": {},
   "source": [
    "#### 2.2.2 Counting categorical variables\n",
    "\n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49860ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Count the number of stores of each type\\nstore_counts = store_types[\"type\"].value_counts()\\nprint(store_counts)\\n\\n# Get the proportion of stores of each type\\nstore_props = store_types[\"type\"].value_counts(normalize=True)\\nprint(store_props)\\n\\n# Count the number of each department number and sort\\ndept_counts_sorted = store_depts[\"department\"].value_counts(sort = True)\\nprint(dept_counts_sorted)\\n\\n# Get the proportion of departments of each number and sort\\ndept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\\nprint(dept_props_sorted)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts(sort = True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769bbe71",
   "metadata": {},
   "source": [
    "### 2.3 Grouped summary statistics\n",
    "\n",
    "    Complex: Summaries by group\n",
    "    \n",
    "        dogs[dogs[\"color\"] == \"Black\"][\"weight_kg\"].mean()\n",
    "        \n",
    "    Grouped summaries\n",
    "    \n",
    "        dogs.groupby(\"color\")[\"weight_kg\"].mean()\n",
    "        \n",
    "    Multiple grouped summaries\n",
    "    \n",
    "        dogs.groupby(\"color\")[\"weight_kg\"].agg([min,max,sum])\n",
    "        \n",
    "    Grouping by multiple variables\n",
    "        dogs.groupby([\"color\", \"breed\"])[\"weight_kg\"].mean()\n",
    "\n",
    "    Many groups, many summaries    \n",
    "        dogs.groupby([\"color\", \"breed\"])[[\"weight_kg\", \"height_cm\"]].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6812942",
   "metadata": {},
   "source": [
    "What percent of sales occurred at each store type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe8e84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Calc total weekly sales\\nsales_all = sales[\"weekly_sales\"].sum()\\n\\n# Subset for type A stores, calc total weekly sales\\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\\n\\n# Subset for type B stores, calc total weekly sales\\nsales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\\n\\n# Subset for type C stores, calc total weekly sales\\nsales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\\n\\n# Get proportion for each type\\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\\nprint(sales_propn_by_type)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e47434",
   "metadata": {},
   "source": [
    "Calculations with .groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0551b2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Group by type; calc total weekly sales\\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\\n\\n# Get proportion for each type\\nsales_propn_by_type =  sales_by_type / sum(sales_by_type )\\nprint(sales_propn_by_type)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type =  sales_by_type / sum(sales_by_type )\n",
    "print(sales_propn_by_type)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b176eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# From previous step\\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\\n\\n# Group by type and is_holiday; calc total weekly sales\\nsales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"])[\"weekly_sales\"].sum()\\nprint(sales_by_type_is_holiday)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca0c98",
   "metadata": {},
   "source": [
    "#### 2.3.1 Multiple grouped summaries\n",
    "\n",
    ".agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af964926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Import numpy with the alias np\\nimport numpy as np \\n\\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\\nsales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([min,max,np.mean,np.median])\\n\\n# Print sales_stats\\nprint(sales_stats)\\n\\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\\nunemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\",\"fuel_price_usd_per_l\"]].agg([min,max,np.mean,np.median])\\n\\n# Print unemp_fuel_stats\\nprint(unemp_fuel_stats)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Import numpy with the alias np\n",
    "import numpy as np \n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([min,max,np.mean,np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\",\"fuel_price_usd_per_l\"]].agg([min,max,np.mean,np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b73dce",
   "metadata": {},
   "source": [
    "### 2.4 Pivot tables\n",
    "\n",
    "    Default\n",
    "\n",
    "        dogs.pivot_table(values = \"weight_kg\", index = \"color\")\n",
    "\n",
    "    Different statistics\n",
    "    \n",
    "        dogs.pivot_table(values = \"weight_kg\", index = \"color\", agg.func = np.median)\n",
    "        \n",
    "        \n",
    "    Pivot on two variables\n",
    "    \n",
    "        dogs.pivot_table(values = \"weight_kg\", index = \"color\", columns = \"breed\")\n",
    "    \n",
    "    Filling missing values in pivot tables\n",
    "    \n",
    "         dogs.pivot_table(values = \"weight_kg\", index = \"color\", columns = \"breed\", fill_value = 0, margins = True)\n",
    "    \n",
    "    Summing with pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e246411f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Pivot for mean weekly_sales for each store type\\nmean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\")\\n\\n# Print mean_sales_by_type\\nprint(mean_sales_by_type)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\")\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd694dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import NumPy as np\\nimport numpy as np\\n\\n# Pivot for mean and median weekly_sales for each store type\\nmean_med_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean, np.median])\\n\\n# Print mean_med_sales_by_type\\nprint(mean_med_sales_by_type)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e26a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Pivot for mean weekly_sales by store type and holiday \\nmean_sales_by_type_holiday = sales.pivot_table(values = \"weekly_sales\", index = \"type\", columns = \"is_holiday\")\\n\\n# Print mean_sales_by_type_holiday\\nprint(mean_sales_by_type_holiday)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values = \"weekly_sales\", index = \"type\", columns = \"is_holiday\")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3c112",
   "metadata": {},
   "source": [
    "Fill in missing values and sum values with pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c527e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print mean weekly_sales by department and type; fill missing values with 0\\nprint(sales.pivot_table(values = \"weekly_sales\", index = \"department\", columns = \"type\", fill_value = 0))'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values = \"weekly_sales\", index = \"department\", columns = \"type\", fill_value = 0))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f14bc058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\\nprint(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0, margins = True))'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0, margins = True))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746c4f1",
   "metadata": {},
   "source": [
    "# 3. Slicing and Indexing Dataframes\n",
    "\n",
    "    3.1 Setting a column as the index\n",
    "    \n",
    "        dogs.set_index(\"name\")\n",
    "\n",
    "    3.2 Removing an index\n",
    "    \n",
    "        dogs.reset_index()\n",
    "       \n",
    "    3.3 Dropping an index\n",
    "    \n",
    "        dogs.reset_index(drop = True)\n",
    "\n",
    "    3.4 Indexes make subsetting simpler\n",
    "    \n",
    "        dogs[dogs[\"name\"].isin([\"Bella\", \"Stella\"])] \n",
    "        \n",
    "        dogs_ind.loc[[\"Bella\", \"Stella\"]]\n",
    "\n",
    "    3.5 Index values don't need to be unique\n",
    "    \n",
    "        dogs_ind2 = dogs.set_index(\"breed\")\n",
    "\n",
    "    3.6 Subsetting on duplicated index values\n",
    "         \n",
    "        dogs_ind2.loc[\"Labrador\"]\n",
    "\n",
    "    3.7 Multi-level indexes a.k.a. hierarchical indexes\n",
    "    \n",
    "        dogs_ind3 = dogs.set_index([\"breed\", \"color\"])\n",
    "\n",
    "    3.8 Subset the outer level with a list\n",
    "    \n",
    "        dogs_ind3 = dogs_ind3.loc[[\"Labdrador\", \"Chihuahua\"]]\n",
    "\n",
    "    3.9 Subset inner levels with a list of tuples\n",
    "    \n",
    "        dogs_ind3 = dogs_ind3.loc[[(\"Labdrador\", \"Brown\"), (\"Chihuahua\", \"Tan\")]]\n",
    "\n",
    "    3.10 Sorting by index values\n",
    "    \n",
    "        dogs_ind3.sort_index()\n",
    "\n",
    "    3.11 Controlling sort_index\n",
    "    \n",
    "        dogs_ind3.sort_index(level = [\"color\", \"breed\"], ascending = [True, False])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07d199",
   "metadata": {},
   "source": [
    "Pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c806c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Look at temperatures\\nprint(temperatures)\\n\\n# Set the index of temperatures to city\\ntemperatures_ind = temperatures.set_index(\"city\")\\n\\n# Look at temperatures_ind\\nprint(temperatures_ind)\\n\\n# Reset the temperatures_ind index, keeping its contents\\nprint(temperatures_ind.reset_index())\\n\\n# Reset the temperatures_ind index, dropping its contents\\nprint(temperatures_ind.reset_index(drop = True))\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop = True))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435606d",
   "metadata": {},
   "source": [
    "#### 3.1 Subsetting with .loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954daf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Make a list of cities to subset on\\ncities = [\"Moscow\", \"Saint Petersburg\"]\\n\\n# Subset temperatures using square brackets\\nprint(temperatures[temperatures[\"city\"].isin(cities)])\\n\\n# Subset temperatures_ind using .loc[]\\nprint(temperatures_ind.loc[cities])\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808e1d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Index temperatures by country & city\\ntemperatures_ind = temperatures.set_index([\"country\", \"city\"])\\n\\n# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\\nrows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\\n\\n# Subset for rows to keep\\nprint(temperatures_ind.loc[rows_to_keep])'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4c3ed",
   "metadata": {},
   "source": [
    "Magnificent multi-level indexing! Multi-level indexes can make it easy to comprehend your dataset when one category is nested inside another category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027fa13",
   "metadata": {},
   "source": [
    "#### 3.1.1 Sorting by index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd80ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sort temperatures_ind by index values\\nprint(temperatures_ind.sort_index())\\n\\n# Sort temperatures_ind by index values at the city level\\nprint(temperatures_ind.sort_index(level = \"city\"))\\n\\n# Sort temperatures_ind by country then descending city\\nprint(temperatures_ind.sort_index(level = [\"country\", \"city\"], ascending = [True, False]))'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level = \"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level = [\"country\", \"city\"], ascending = [True, False]))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f1d86",
   "metadata": {},
   "source": [
    "### 3.2 Slicing and subsetting with .loc and .iloc\n",
    "\n",
    "    A. LOC\n",
    "\n",
    "    3.2.1 Sort the index before you slice\n",
    "    \n",
    "        dogs_srt = dogs.set_index([\"breed\", \"color\"]).sort_index()\n",
    "        print(dogs_srt)\n",
    "    \n",
    "    3.2.2 Slicing the outer index level which cannot be done in the inner index\n",
    "    \n",
    "        dogs_srt.loc[\"Chow Chow\": \"Poodle\"] \n",
    "    \n",
    "    3.2.3 Slicing the inner index level \n",
    "    \n",
    "        dogs_srt.loc[(\"Labrador\", \"Brown\"):(\"Schnauzer\", \"Grey\")]\n",
    "        \n",
    "    3.2.4 Slice twice         \n",
    "    \n",
    "        dogs_srt.loc[(\"Labrador\", \"Brown\"):(\"Schnauzer\", \"Grey\"), (\"name\":\"heigh_cm\")]\n",
    "        \n",
    "        \n",
    "    3.2.5 Slicing by dates\n",
    "    \n",
    "        dogs_srt.loc['2014-08-25:\"2016-09-16\"]\n",
    "             \n",
    "    3.2.6 Slicing by partial dates \n",
    "    \n",
    "        dogs_srt.loc[\"2014\":\"2016\"]\n",
    "        \n",
    "    \n",
    "    B. ILOC\n",
    "    \n",
    "    Subsetting by row/column number\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d316cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sort the index of temperatures_ind\\ntemperatures_srt = temperatures_ind.sort_index()\\n\\n# Subset rows from Pakistan to Russia\\nprint(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\\n\\n# Try to subset rows from Lahore to Moscow\\nprint(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\\n\\n# Subset rows from Pakistan, Lahore to Russia, Moscow\\nprint(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\",\"Moscow\")])'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\",\"Moscow\")])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4698cd0",
   "metadata": {},
   "source": [
    "#### 3.2.4 Slicing in both directions\n",
    "\n",
    "    Slice rows with code like df.loc[(\"a\", \"b\"):(\"c\", \"d\")].\n",
    "    \n",
    "    Slice columns with code like df.loc[:, \"e\":\"f\"].\n",
    "    \n",
    "    Slice both ways with code like df.loc[(\"a\", \"b\"):(\"c\", \"d\"), \"e\":\"f\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc8b6a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Subset rows from India, Hyderabad to Iraq, Baghdad\\nprint(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\\n\\n# Subset columns from date to avg_temp_c\\nprint(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\\n\\n# Subset in both directions at once\\nprint(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5d458",
   "metadata": {},
   "source": [
    "#### 3.2.5 Slicing Time Series\n",
    "\n",
    "    Slicing is particularly useful for time series since it's a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, \"yyyy-mm-dd\" for year-month-day, \"yyyy-mm\" for year-month, and \"yyyy\" for year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6efa0666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\\ntemperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\\nprint(temperatures_bool)\\n\\n# Set date as the index and sort the index\\ntemperatures_ind = temperatures.set_index(\"date\").sort_index()\\n\\n# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\\nprint(temperatures_ind.loc[\"2010\":\"2011\"])\\n\\n# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\\nprint(temperatures_ind.loc[\"2010-08\":\"2011-02\"])\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010\":\"2011\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"2010-08\":\"2011-02\"])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65044326",
   "metadata": {},
   "source": [
    "#### 3.2.6 Subsetting by row/column number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2cc170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Get 23rd row, 2nd column (index 22, 1)\\nprint(temperatures.iloc[22,1])\\n\\n# Use slicing to get the first 5 rows\\nprint(temperatures.iloc[:5])\\n\\n# Use slicing to get columns 3 to 4\\nprint(temperatures.iloc[:,2:4])\\n\\n# Use slicing in both directions at once\\nprint(temperatures.iloc[:5,2:4])'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22,1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:,2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[:5,2:4])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ccc6a",
   "metadata": {},
   "source": [
    "### 3.3 Working with Pivot Tables\n",
    "\n",
    "    3.3.1 - .loc[] + slicing is a power combo\n",
    "    \n",
    "        dogs_height_by_breed_vs_color.loc[\"Chow Chow\": \"Poodle\"]\n",
    "        \n",
    "    3.3.2 The axis argument - summ stats across rows \n",
    "        \n",
    "        dogs_height_by_breed_vs_color.mean(axis = \"index\")\n",
    "        \n",
    "        dogs_height_by_breed_vs_color.mean(axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10804eba",
   "metadata": {},
   "source": [
    "#### 3.3.1 Pivot temperature by city and year\n",
    "\n",
    "    You can access the components of a date (year, month and day) using code of the form dataframe[\"column\"].dt.component. For example, the month component is dataframe[\"column\"].dt.month, and the year component is dataframe[\"column\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c85badc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Add a year column to temperatures\\ntemperatures[\"year\"] = temperatures[\"date\"].dt.year\\n\\n# Pivot avg_temp_c by country and city vs year\\ntemp_by_country_city_vs_year = temperatures.pivot_table(values = \"avg_temp_c\", index =[\"country\", \"city\"], columns = \"year\")\\n\\n# See the result\\nprint(temp_by_country_city_vs_year)\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Add a year column to temperatures\n",
    "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(values = \"avg_temp_c\", index =[\"country\", \"city\"], columns = \"year\")\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4389a70",
   "metadata": {},
   "source": [
    "#### 3.3.2 Subsetting pivot tables\n",
    "\n",
    "    A pivot table is just a DataFrame with sorted indexes, so the techniques you have learned already can be used to subset them. In particular, the .loc[] + slicing combination is often helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd3bec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Subset for Egypt to India\\ntemp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\\n\\n# Subset for Egypt, Cairo to India, Delhi\\ntemp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\",\"Delhi\")]\\n\\n# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\\ntemp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\",\"Delhi\"), \"2005\": \"2010\"]\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Subset for Egypt to India\n",
    "temp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\",\"Delhi\")]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\",\"Delhi\"), \"2005\": \"2010\"]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b2130",
   "metadata": {},
   "source": [
    "#### 3.3.3 Calculating on a pivot table\n",
    "\n",
    "Find the rows or columns where the highest or lowest value occurs.\n",
    "\n",
    "Recall from Chapter 1 that you can easily subset a Series or DataFrame to find rows of interest using a logical condition inside of square brackets. For example: series[series > value]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5788955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Get the worldwide mean temp by year\\nmean_temp_by_year = temp_by_country_city_vs_year.mean()\\n\\n# Filter for the year that had the highest mean temp\\nprint(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\\n\\n# Get the mean temp by city\\nmean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\\n\\n# Filter for the city that had the lowest mean temp\\nprint(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed24ea",
   "metadata": {},
   "source": [
    "## 4. Creating and Visualizing DataFrames\n",
    "\n",
    "4.1 Visualizing your data \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    4.1.1 Histogram \n",
    "       x[\"var\"].hist(bins=5)\n",
    "       plt.show()\n",
    "       \n",
    "    4.1.2 Barplots\n",
    "    \n",
    "        avg_weight_by_breed = dog_pack.groupby(\"breed\")[\"weight_kg\"].mean()\n",
    "        print(avg_weight_by_breed)\n",
    "        \n",
    "        avg_weight_by_breed.plot(kind = \"bar\", title = \"   \")\n",
    "        plt.show\n",
    "        \n",
    "    4.1.3 Lineplot\n",
    "    \n",
    "        sully.plot(x = \"date\", y = \"weight_kg\", kind = \"line\", rot = 45)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    4.1.4 Scatterplot\n",
    "    \n",
    "        sully.plot(x = \"date\", y = \"weight_kg\", kind = \"scatter\")\n",
    "        \n",
    "    \n",
    "    4.1.5 Layering plots\n",
    "    \n",
    "        dog_pack[dog_pack[\"sex\"] == \"F\"][\"height_cm\"].hist.hist(alpha = 0.7)\n",
    "        dog_pack[dog_pack[\"sex\"] == \"M\"][\"height_cm\"].hist.hist(alpha = 0.7)\n",
    "        plt.legend([\"F\", \"M\"])\n",
    "        plt.show()\n",
    "    \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047dbd9",
   "metadata": {},
   "source": [
    "#### 4.1.1 Which avocado size is most popular?\n",
    "\n",
    "Bar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but you'll often have to manipulate your data first in order to get the numbers you need for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4750b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import matplotlib.pyplot with alias plt\\nimport matplotlib.pyplot as plt\\n\\n# Look at the first few rows of data\\nprint(avocados.head())\\n\\n# Get the total number of avocados sold of each size\\nnb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\\n\\n\\n# Create a bar plot of the number of avocados sold by size\\nnb_sold_by_size.plot(kind=\"bar\")\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\n",
    "\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind=\"bar\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2a499",
   "metadata": {},
   "source": [
    "#### 4.1.2 Changes in sales over time\n",
    "\n",
    "Line plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing the change in a number over time since each time point is naturally connected to the next time point. In this exercise, you'll visualize the change in avocado sales over three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98939608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import matplotlib.pyplot with alias plt\\nimport matplotlib.pyplot as plt\\n\\n# Get the total number of avocados sold on each date\\nnb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\\n\\n# Create a line plot of the number of avocados sold by date\\nnb_sold_by_date.plot(kind = \"line\")\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind = \"line\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63df9a",
   "metadata": {},
   "source": [
    "#### 4.1.3 Avocado supply and demand\n",
    "\n",
    "Scatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you'll compare the number of avocados sold to average price and see if they're at all related. If they're related, you may be able to use one number to predict the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28630a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Scatter plot of avg_price vs. nb_sold with title\\navocados.plot(x= \"nb_sold\", y = \"avg_price\", kind = \"scatter\", title = \"Number of avocados sold vs. average price\")\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Scatter plot of avg_price vs. nb_sold with title\n",
    "avocados.plot(x= \"nb_sold\", y = \"avg_price\", kind = \"scatter\", title = \"Number of avocados sold vs. average price\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d902c",
   "metadata": {},
   "source": [
    "#### 4.1.4 Price of conventional vs. organic avocados\n",
    "\n",
    "Creating multiple plots for different subsets of data allows you to compare groups. In this exercise, you'll create multiple histograms to compare the prices of conventional and organic avocados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1043abd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Modify bins to 20\\navocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\\n\\n# Modify bins to 20\\navocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\\n\\n# Add a legend\\nplt.legend([\"conventional\", \"organic\"])\\n\\n# Show the plot\\nplt.show()\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e07b1",
   "metadata": {},
   "source": [
    "On average, organic avocados are more expensive than conventional ones, but their price distributions have some overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44aba0",
   "metadata": {},
   "source": [
    "### 4.2 MISSING VALUES \n",
    "\n",
    "    4.2.1 Detecting missing values\n",
    "    \n",
    "        dogs.isna() - Boolean for every single value indicating whether the value is missing or not, but this isn't very helpful when you're working with a lot of data.\n",
    "\n",
    "        dogs.isna().any() - one value for each variable that tells us if there are any missing values in that column.\n",
    "    \n",
    "    \n",
    "    4.2.1 Counting missing values\n",
    "\n",
    "        dogs.isna().sum()\n",
    "    \n",
    "    4.2.2 Plotting missing values\n",
    "    \n",
    "        dogs.isna().sum().plot(kind = \"bar\")\n",
    "        plt.show()\n",
    "        \n",
    "    4.2.3 Removing missing values \n",
    "    \n",
    "        dogs.dropna()\n",
    "        dogs.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db31b3",
   "metadata": {},
   "source": [
    "#### 4.2.1 Finding missing values\n",
    "\n",
    "Missing values are everywhere, and you don't want them interfering with your work. Some functions ignore missing data by default, but that's not always the behavior you might want. Some functions can't handle missing values at all, so these values need to be taken care of before you can use them. If you don't know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you'll determine if there are missing values in the dataset, and if so, how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdd1656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Import matplotlib.pyplot with alias plt\\nimport matplotlib.pyplot as plt\\n\\n# Check individual values for missing values\\nprint(avocados_2016.isna())\\n\\n# Check each column for missing values\\n\\nprint(avocados_2016.isna().any())\\n# Bar plot of missing values by variable\\nprint(avocados_2016.isna().sum().plot(kind = \"bar\"))\\n\\n# Show plot\\nplt.show()\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "\n",
    "print(avocados_2016.isna().any())\n",
    "# Bar plot of missing values by variable\n",
    "print(avocados_2016.isna().sum().plot(kind = \"bar\"))\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fe110",
   "metadata": {},
   "source": [
    "missing values in the small_sold, large_sold, and xl_sold columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750e0fc",
   "metadata": {},
   "source": [
    "#### 4.2.2 Removing missing values\n",
    "\n",
    "One way is to remove them from the dataset completely. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "177a0c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Remove rows with missing values\\navocados_complete = avocados_2016.dropna()\\n\\n# Check if any columns contain missing values\\nprint(avocados_complete.isna().any())'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8654d4",
   "metadata": {},
   "source": [
    "Removing observations with missing values is a quick and dirty way to deal with missing data, but this can introduce bias to your data if the values are not missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93fc478",
   "metadata": {},
   "source": [
    "#### 4.2.3 Replacing missing values\n",
    "\n",
    "For numerical variables, one option is to replace values with 0— you'll do this here. However, when you replace missing values, you make assumptions about what a missing value means. In this case, you will assume that a missing number sold means that no sales for that avocado type were made that week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fe81c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# List the columns with missing values\\ncols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\\n\\n# Create histograms showing the distributions cols_with_missing\\navocados_2016[cols_with_missing].hist()\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions cols_with_missing\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "844b2d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# From previous step\\ncols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\\navocados_2016[cols_with_missing].hist()\\nplt.show()\\n\\n# Fill in missing values with 0\\navocados_filled = avocados_2016.fillna(0)\\n\\n# Create histograms of the filled columns\\navocados_filled[cols_with_missing].hist()\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bcef8",
   "metadata": {},
   "source": [
    "### 4.3 Creating DataFrames \n",
    "\n",
    "Dictionaries\n",
    "\n",
    "    my_dict = {\"key1\": value1}\n",
    "    my_dict[\"key1\"]\n",
    "    \n",
    "    \n",
    "Creating Dataframes\n",
    "\n",
    "    From a list of dictionaries: row by row\n",
    "    \n",
    "        list_of_dicts = [{\"name\": \"Ginger\" , \"breed\": \"Dashshund\"},{\"name\": \"Scout\" , \"breed\": \"Dalmatian\"}]\n",
    "        new_dogs = pd.DataFrame(list_of_dicts)\n",
    "        print(new_dogs)\n",
    "\n",
    "    From a dictionary of lists: column by column \n",
    "    \n",
    "    dict_of_lists = {\"name\": [\"Ginger\", \"Scout\"], \"breed\": [\"Dachshund\", \"Dalmatian\"]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a736ed7",
   "metadata": {},
   "source": [
    "#### 4.3.1 List of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2282b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create a list of dictionaries with new data\\navocados_list = [\\n    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \\'large_sold\\': 7835071},\\n    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \\'large_sold\\': 8561348},\\n]\\n\\n# Convert list into DataFrame\\navocados_2019 = pd.DataFrame(avocados_list)\\n\\n# Print the new DataFrame\\nprint(avocados_2019)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, 'large_sold': 7835071},\n",
    "    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, 'large_sold': 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d6e99",
   "metadata": {},
   "source": [
    "#### 4.3.2 Dictionaries of List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a2ae7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create a dictionary of lists with new data\\navocados_dict = {\\n  \"date\": [\"2019-11-17\", \"2019-12-01\" ],\\n  \"small_sold\": [10859987, 9291631],\\n  \"large_sold\": [7674135, 6238096]\\n}\\n\\n# Convert dictionary into DataFrame\\navocados_2019 = pd.DataFrame(avocados_dict)\\n\\n# Print the new DataFrame\\nprint(avocados_2019)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "  \"date\": [\"2019-11-17\", \"2019-12-01\" ],\n",
    "  \"small_sold\": [10859987, 9291631],\n",
    "  \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5a8fa",
   "metadata": {},
   "source": [
    "## 4.4 Reading and writing CSVs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6679d",
   "metadata": {},
   "source": [
    "CSV, or comma-separated values, is a common data storage file type. It's designed to store tabular data, just like a pandas DataFrame. It's a text file, where each row of data has its own line, and each value is separated by a comma.\n",
    "    \n",
    "    CSV to DataFrame\n",
    "    \n",
    "        import pandas as pd\n",
    "\n",
    "        new_dogs = pd.read_csv(\"new_dogs.csv\")\n",
    "\n",
    "    DataFrame manipulation\n",
    "    \n",
    "        new_dogs[\"bmi\"] = new_dogs[\"weight]/(ew_dogs[height]/100)**2\n",
    "    \n",
    "    DataFrame to CSV\n",
    "    \n",
    "        new_dogs.to_csv(\"new_dogs_with_bmi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79f279",
   "metadata": {},
   "source": [
    "#### 4.4.1 CSV to DataFrame\n",
    "\n",
    "You work for an airline, and your manager has asked you to do a competitive analysis and see how often passengers flying on other airlines are involuntarily bumped from their flights. You got a CSV file (airline_bumping.csv) from the Department of Transportation containing data on passengers that were involuntarily denied boarding in 2016 and 2017, but it doesn't have the exact numbers you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7addfa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# From previous steps\\n    airline_bumping = pd.read_csv(\"airline_bumping.csv\")\\n    print(airline_bumping.head())\\n    airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\\n    airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\\n\\n# Print airline_totals\\n    print(airline_totals)\\n\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# From previous steps\n",
    "    airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "    print(airline_bumping.head())\n",
    "    airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "    airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
    "\n",
    "# Print airline_totals\n",
    "    print(airline_totals)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12edff11",
   "metadata": {},
   "source": [
    "#### 4.4.2 DataFrame to CSV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dfeec69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create airline_totals_sorted\\n    airline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending = False)\\n\\n# Print airline_totals_sorted\\n    print(airline_totals_sorted)\\n\\n# Save as airline_totals_sorted.csv\\nairline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")\\n\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create airline_totals_sorted\n",
    "    airline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending = False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "    print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
